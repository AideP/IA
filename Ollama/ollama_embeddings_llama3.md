
# ğŸ§  Comprendiendo los Embeddings, Ollama y Llama 3.2

## ğŸ“Œ Â¿QuÃ© son los *embeddings*?

Los **embeddings** son representaciones numÃ©ricas de textos en forma de vectores. Su funciÃ³n principal es traducir palabras, frases o documentos en nÃºmeros que conserven su significado semÃ¡ntico, permitiendo que una computadora compare similitudes entre textos.

Por ejemplo:
- *â€œabortoâ€* y *â€œinterrupciÃ³n del embarazoâ€* deberÃ­an tener vectores cercanos porque significan lo mismo.
- Esto permite realizar bÃºsquedas semÃ¡nticas, no solo por palabras exactas.

> En este proyecto usamos el modelo `nomic-embed-text` para generar estos embeddings usando Ollama localmente.

---

## âš™ï¸ Â¿CÃ³mo se generan y usan los embeddings en este sistema?

### 1. **IndexaciÃ³n de textos** (`indexar_textos.py`):
- Se leen archivos `.txt` desde carpetas temÃ¡ticas (ej. `aborto`, `eutanasia`).
- Cada texto se envÃ­a a `http://localhost:11434/api/embeddings` usando el modelo `nomic-embed-text`.
- El vector de embedding devuelto se guarda en una base de datos local (`ChromaDB`) junto con el texto y metadatos.

### 2. **Consulta de informaciÃ³n** (`preguntar.py`):
- El usuario escribe una pregunta.
- Esta pregunta se transforma tambiÃ©n en un embedding.
- Se busca el vector mÃ¡s cercano dentro de los documentos ya indexados.
- Se seleccionan los textos mÃ¡s relevantes para armar un *contexto*.
- Ese contexto + pregunta se pasan como *prompt* a un modelo LLM (en este caso `llama3`).

---

## ğŸ§  Â¿QuÃ© es Ollama?

**Ollama** es una herramienta que permite ejecutar modelos de lenguaje grandes (*Large Language Models*) localmente, sin depender de la nube.

**Ventajas:**
- No necesitas conexiÃ³n a OpenAI ni otras APIs externas.
- Puedes usar modelos como LLaMA, Mistral, Gemma, entre otros.
- Accedes a endpoints REST como `/api/generate` y `/api/embeddings`.

En este proyecto usamos Ollama para dos tareas:
- Generar *embeddings* (`/api/embeddings`)
- Generar respuestas (`/api/generate`)

---

## ğŸ¦™ Â¿QuÃ© es LLaMA 3.2 (llama3)?

**LLaMA** (Large Language Model Meta AI) es una familia de modelos de lenguaje desarrollada por **Meta (Facebook)**. En su versiÃ³n 3.2, ofrece mejoras significativas en comprensiÃ³n, velocidad y respuesta natural del lenguaje.

CaracterÃ­sticas clave de `llama3.2`:
- Capacidad para trabajar localmente vÃ­a Ollama.
- Compatible con prompts de tipo *instrucciÃ³n-respuesta*.
- Puede responder preguntas especÃ­ficas usando contexto personalizado.

---

## ğŸ§ª Flujo general del sistema

```
Usuario (Pregunta) â”€â”€â–¶ [Generar embedding de la pregunta]
                              â”‚
                              â–¼
                 [Buscar documentos similares en ChromaDB]
                              â”‚
                              â–¼
                [Construir contexto + pregunta como prompt]
                              â”‚
                              â–¼
                     [Enviar a llama3 via Ollama]
                              â”‚
                              â–¼
                        ğŸ§  Respuesta generada
```

---

## ğŸ“ Estructura de archivos

```
IA/
â”‚
â”œâ”€â”€ indexar_textos.py    # Indexa textos con embeddings
â”œâ”€â”€ preguntar.py         # Permite hacer preguntas al modelo
â”œâ”€â”€ db_ollama/           # Base de datos local de Chroma (persistente)
â”œâ”€â”€ ollama/              # Carpeta con textos por tema
â”‚   â”œâ”€â”€ aborto/texto_pdf/*.txt
â”‚   â””â”€â”€ eutanasia/texto_pdf/*.txt

```

---

## âœ… Requisitos

- Tener instalado **Ollama**: [https://ollama.com](https://ollama.com)
- Modelos necesarios:
  ```bash
  ollama pull nomic-embed-text
  ollama pull llama3
  ```
- Tener ChromaDB y `requests` instalados:
  ```bash
  pip install chromadb requests
  ```

---

## ğŸ”„ Comandos de ejecuciÃ³n

**Para indexar textos:**
```bash
python indexar_textos.py
```

**Para hacer preguntas:**
```bash
python preguntar.py
```

---

## ğŸ§© Posibles errores y soluciones

| Problema                           | SoluciÃ³n recomendada                                           |
|------------------------------------|----------------------------------------------------------------|
| `list index out of range` en query | AsegÃºrate de que se generen correctamente los embeddings.      |
| Embeddings vacÃ­os                  | Verifica que `nomic-embed-text` estÃ© funcionando en Ollama.    |
| No se encuentra modelo             | Ejecuta `ollama pull nomic-embed-text` o `llama3`              |
| Archivos `.txt` vacÃ­os             | AsegÃºrate de que los textos tengan contenido relevante.        |
